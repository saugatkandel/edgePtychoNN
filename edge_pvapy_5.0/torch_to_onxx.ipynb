{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68042c9-5284-41cd-8357-41b3aff4c7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93759da1-1bcf-4bbd-9cc9-e8b5a1659c29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db793972-31ec-42c7-89a3-716b5f347f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def engine_build_from_onnx(onnx_mdl):\n",
    "    EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.ERROR)\n",
    "    builder = trt.Builder(TRT_LOGGER)\n",
    "    config = builder.create_builder_config()\n",
    "    # config.set_flag(trt.BuilderFlag.FP16)\n",
    "    config.set_flag(trt.BuilderFlag.TF32)\n",
    "    config.max_workspace_size = 1 * (1 << 50)  # the maximum size that any layer in the network can use\n",
    "\n",
    "    network = builder.create_network(EXPLICIT_BATCH)\n",
    "    parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "    # Load the Onnx model and parse it in order to populate the TensorRT network.\n",
    "    success = parser.parse_from_file(onnx_mdl)\n",
    "\n",
    "    for idx in range(parser.num_errors):\n",
    "        print(parser.get_error(idx))\n",
    "\n",
    "    if not success:\n",
    "        return None\n",
    "\n",
    "    return builder.build_engine(network, config)\n",
    "\n",
    "\n",
    "def mem_allocation(engine):\n",
    "    # Determine dimensions and create page-locked memory buffers (i.e. won't be swapped to disk) to hold host inputs/outputs.\n",
    "\n",
    "    in_sz = trt.volume(engine.get_binding_shape(0)) * engine.max_batch_size\n",
    "    h_input = cuda.pagelocked_empty(in_sz, dtype=\"float32\")\n",
    "\n",
    "    out_sz = trt.volume(engine.get_binding_shape(1)) * engine.max_batch_size\n",
    "    h_output = cuda.pagelocked_empty(out_sz, dtype=\"float32\")\n",
    "\n",
    "    # Allocate device memory for inputs and outputs.\n",
    "    d_input = cuda.mem_alloc(h_input.nbytes)\n",
    "    d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "\n",
    "    # Create a stream in which to copy inputs/outputs and run inference.\n",
    "    stream = cuda.Stream()\n",
    "\n",
    "    return h_input, h_output, d_input, d_output, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356b8956-29e4-4924-a640-19c4ba4fe085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReconSmallPhaseModel(nn.Module):\n",
    "    def __init__(self, nconv: int = 32):\n",
    "        super(ReconSmallPhaseModel, self).__init__()\n",
    "        self.nconv = nconv\n",
    "\n",
    "        self.encoder = nn.Sequential(  # Appears sequential has similar functionality as TF avoiding need for separate model definition and activ\n",
    "            *self.down_block(1, self.nconv),\n",
    "            *self.down_block(self.nconv, self.nconv * 2),\n",
    "            *self.down_block(self.nconv * 2, self.nconv * 4),\n",
    "            *self.down_block(self.nconv * 4, self.nconv * 8),\n",
    "            *self.down_block(self.nconv * 8, self.nconv * 16),\n",
    "            *self.down_block(self.nconv * 16, self.nconv * 32),\n",
    "        )\n",
    "\n",
    "        # amplitude model\n",
    "        # self.decoder1 = nn.Sequential(\n",
    "        #    *self.up_block(self.nconv * 32, self.nconv * 16),\n",
    "        #    *self.up_block(self.nconv * 16, self.nconv * 8),\n",
    "        #    *self.up_block(self.nconv * 8, self.nconv * 4),\n",
    "        #   *self.up_block(self.nconv * 4, self.nconv * 2),\n",
    "        #    *self.up_block(self.nconv * 2, self.nconv * 1),\n",
    "        #    *self.up_block(self.nconv * 1, 16),\n",
    "        #    nn.Conv2d(16 , 1, 3, stride=1, padding=(1,1)),\n",
    "        #    nn.Tanh()\n",
    "        # )\n",
    "\n",
    "        # phase model\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            *self.up_block(self.nconv * 32, self.nconv * 16),  # 16\n",
    "            *self.up_block(self.nconv * 16, self.nconv * 8),  # 32\n",
    "            *self.up_block(self.nconv * 8, self.nconv * 4),  # 64\n",
    "            *self.up_block(self.nconv * 4, self.nconv * 2),  # 128\n",
    "            # *self.up_block(self.nconv * 2, self.nconv * 1),\n",
    "            # *self.up_block(self.nconv * 1, 16),\n",
    "            nn.Conv2d(self.nconv * 2, 1, 3, stride=1, padding=(1, 1)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def down_block(self, filters_in, filters_out):\n",
    "        block = [\n",
    "            nn.Conv2d(\n",
    "                in_channels=filters_in,\n",
    "                out_channels=filters_out,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=(1, 1),\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filters_out, filters_out, 3, stride=1, padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "        ]\n",
    "        return block\n",
    "\n",
    "    def up_block(self, filters_in, filters_out):\n",
    "        block = [\n",
    "            nn.Conv2d(filters_in, filters_out, 3, stride=1, padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filters_out, filters_out, 3, stride=1, padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\"),\n",
    "        ]\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            x1 = self.encoder(x)\n",
    "            # amp = self.decoder1(x1)\n",
    "            ph = self.decoder2(x1)\n",
    "\n",
    "            # Restore -pi to pi range\n",
    "            ph = (\n",
    "                ph * np.pi\n",
    "            )  # Using tanh activation (-1 to 1) for phase so multiply by pi\n",
    "\n",
    "        return ph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d94aee-fac4-435f-99ff-1b1b15668080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bsz = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30172c3c-dfd6-48e3-91f9-e49b3f5441d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = \"/home/beams/SKANDEL/beamtime_data/sector26_02_28_23/ptychonn_02_28_23/iteration_03_02_04_00/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0063e4a-6c9b-4389-800b-2d05a3aa8030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beams/SKANDEL/miniforge3/envs/edge2/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1673730874951/work/torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/beams/SKANDEL/miniforge3/envs/edge2/lib/python3.10/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1673730874951/work/torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/beams/SKANDEL/miniforge3/envs/edge2/lib/python3.10/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1673730874951/work/torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "#model_path = \"/home/beams/SKANDEL/code/anakha_ptychoNN-test/models_11_22/best_model_reduced_model.pth\"\n",
    "model_path = f\"{base_path}/best_model.pth\"\n",
    "model = ReconSmallPhaseModel()\n",
    "   \n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "#summary(model, (1, 1, 512, 512))\n",
    "\n",
    "dummy_input = torch.randn(bsz, 1, 512, 512)  # batchsize , 1, h, w\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    f\"{base_path}/best_model_bsz_{bsz}.onnx\",\n",
    "    opset_version=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a4290a2-2597-4752-bcbb-13fe5bbb7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/home/beams/SKANDEL/beamtime_data/sector26_02_28_23/Training5/scan168.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4636e67d-bf93-472a-9987-158e965af877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2607390/4054450994.py:8: DeprecationWarning: Use set_memory_pool_limit instead.\n",
      "  config.max_workspace_size = 1 * (1 << 50)  # the maximum size that any layer in the network can use\n",
      "/tmp/ipykernel_2607390/4054450994.py:21: DeprecationWarning: Use build_serialized_network instead.\n",
      "  return builder.build_engine(network, config)\n"
     ]
    }
   ],
   "source": [
    "engine = engine_build_from_onnx(\"/home/beams/SKANDEL/beamtime_data/sector26_02_28_23/ptychonn_02_28_23/iteration_03_01_13_19/best_model_bsz_8.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94316cc2-30d8-446c-bbe6-5340e15c661c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge2",
   "language": "python",
   "name": "edge2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
